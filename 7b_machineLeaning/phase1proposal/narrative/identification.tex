\section{Identification and Significance of the Problem or Opportunity, and Technical Approach}
%% {\em Define the specific technical problem or opportunity addressed by
%%  your application.  Provide enough background information so that the
%%  importance of the problem/opportunity is clear.  Indicate the overall
%%  technical approach to the problem/opportunity and the part that the
%%  proposed research plays in providing needed results.}

RNET Technologies Inc. (RNET) in Dayton, OH and Professor Boyana Norris from the University of Oregon (UO) are responding to the 2019 DOE 
SBIR/STTR (Release 1) Topic 7b: Technologies for Extreme Scale Computing (Software). RNET and UO are proposing the development of the Smart Algorithm Selection through Inference (SASI) toolkit; a machine learning framework for the run-time optimization of numerical algorithms in terms of performance metrics such as CPU-time, memory consumption, energy usage, and resilience. The SASI framework will be built based on the ideas, workflows and results obtained in the projects team previous work developing machine learning models for optimal algorithm selection in linear solver packages such as PETSc. In addition to this ongoing work, RNET has extensive experience in various aspects of High Performance Computing such as performance 
optimization of numerical softwares and libraries, development of fine-grained power monitoring tools 
for HPC infrastructure and large scale data analysis tools. Dr. Boyana Norris (UO) has extensive experience 
in enabling technologies for high-performance simulations in computational science and engineering. 
Specifically, her research on performance analysis of scientific codes spans a spectrum of topics, 
including static analysis, runtime performance monitoring, performance database management, postmortem 
performance analysis, and source transformation tools for performance tuning. Therefore, 
this team is well positioned to develop and commercialize the proposed smart algorithm selection package for 
large-scale numerical simulation. 


\subsection{Identification and Significance}
\label{sec:introduction}

In an effort to further improve the nations numerical simulation capabilities, the DOE, academia and other government agencies have invested heavily in the development of efficient algorithms and implementations for solving the broad spectrum of subproblems that arise in numerical simulation.  The result of this investment is a robust collection of software packages designed to solve numerical subproblems across a broad spectrum of applications and compute architectures. One only needs to look at the expansive collection of linear solvers and preconditioners available in PETSc to see the shear volume of implementations and algorithms that have been developed to solve just one, albeit important, class of numerical subproblem.  

While much work has taken place in optimizing the algorithms and implementations for specific problems and architectures, there is no simple governing theory for choosing between the numerous algorithms and configurations. Rather, the optimal method is, in practice, determined by experimentation and numerical folklore~\cite{EijkFuen2010:multistage}. The experience and expertise provided by the domain scientists in tuning the algorithms cannot be underestimated, however, it is desirable to have toolkit-enabled functionality that automatically chooses the algorithm that is both appropriate for the computational problem at hand \emph{and} the computer architecture that will be used. Automatic selection of an appropriate algorithm and configuration can lead to benefits such as reduced memory requirement, lower execution time, fewer synchronization points in a parallel computation and so forth. 

As an example, consider the large sparse linear systems that arise when solving a nonlinear partial differential equation. One could take a guess as to the best linear solver based on coarse grained problem characteristics and experience, however, it is all but impossible to robustly determine the best solver in all cases. In fact, it has been shown that there is no uniformly optimal linear solver for general matrix-vector systems and that as the simulation progresses and the linear systems produced by the simulation changes, the best preconditioned iterative solver to solve each linear system can also change \cite{Eller2012}. Similarly, in applications with dynamic mesh adaptation, runtime changes in the structure of the mesh (both physical and geometrical) have also been shown to lead to changes in the linear system and hence, the optimal linear solver. The memory constraints of the solver can also change according to the code implementation or the architecture of the machine, highlighting the fact that the compute architecture also plays a huge role in the performance of different algorithms. In other words, for optimal performance, the choice of linear solver should not be a static, a-priori design parameter, but rather, a dynamic, architecture dependent decision made based on the properties of the problem and the performance goals of the user. The same argument holds true for the vast majority of numerical subproblem solvers, including eigensolvers, nonlinear solvers, graph algorithms and nonlinear optimization routines. 

To address this need, RNET and the University of Oregon are proposing the Smart Algorithm selection through Inference (SASI) framework. The idea for SASI was born out of the ideas, lessons and workflows developed during the project teams previous work on automatic solver selection for linear solvers, eigen-solvers and graph algorithms. That previous work has continuously shown that SASI style performance models that form the foundation of the SASI framework are capable of repeatedly predicting, with an extremely high accuracy (i.e., 95\%+), the performance of numerical algorithms and solver configurations across a wide range of applications and domains.

The typical workflow for a developer and end-user of advanced numerical simulations involves testing, development and production runs on a wide range on computational resources, ranging from laptops and locally hosted clusters, through to leadership class HPC resources. SASI models allow developers to create simulations that automatically adapt to both the arcitecture and the problem, removing the need for architecture and problem specific tuning, ultimately reducing the overall time to solution for numerical simulation and ensuring the effcient usage of the nations HPC resources.  


\subsubsection{Smart Algorithm Selection Through Inference }

The proposed framework will guide the user through the process of building and using smart algorithm selection models for performance optimization in terms of metrics such as CPU-time, memory usage, energy efficiency and resillency, automating the process whereever possible. Such optimizations will allow developers to deploy simulations that are optimized to perform within the constraints of the given architecture; a feature that will be of particular importance on exascale machines, where factors such as excessive power consumption and and poor algorithmic resillency are predicted to cause major problems.

The model building pipeline the SASI framework will look to automate can be split into four distinct components; feature determination, data collection, model building and validation and efficient software integration, each of which is rife with intricate details and nuances that must be addressed to ensure optimal performance in the final simulation:

\begin{itemize}
 \item {\bf Feature Determination:} In machine learning terminology, a feature is any obtainable metric, measurement or observation that can be made about the target phenomenon. Determination of an informative, discriminating and independent set of features is a crucial step in the creation of unbiased and useful classification and regression based machine learning algorithms such as those used in SASI.  In SASI, a feature represents a computable metric of the computational model (i.e, the linear system in a linear solver or the graph in graph solvers), that can be used to indicate or predict the performance of an algorithm when applied to a specific problem and on a specific architecture. The process of determining the best features for a given algorithm must be completed in collaboration with a domain scientist and cannot be automated. However, there are numerous feature set optimization techniques that can be used to enhance the perfromance of the SASI models, all of which will be included in SASI.  
 
 \item {\bf Data Collection:} A large diverse training data set forms the foundation of every machine learning model. In the context of SASI, training data takes the form of a set of measurements obtained by applying the different algorithms and configurations to a diverse set of sample problems (e.g., for linear solvers, this means solving a large number of matrix-vector systems using a wide variety of linear solvers, preconditioners and input parameters.) Collecting this data is the most expensive component in building a machine learning model, and again, requires a domain scientist famillar with a wide range of applications for the given algorithm. To aid in this process, SASI will include a robust database management system, along with several novel tools for assessing the validity and suitability of the training data.  
 
 \item {\bf Model selection and Validation:} The third component of the SASI workflow involves choosing a machine learning algorithm, building the model, and testing the result. Previous results have shown that simple machine learning algorithms such as a single decision tree or a Random forest can be used to create cheap and highly accurate machine learning models for smart algorithm selection. For instance, the decision tree based J48 algorithm was the most accurate algorithm for the linear solver based SASI models developed in previous work. SASI will include a modular, extensible interface for connecting to external machine learning toolkits that takes care of building, serializing and loading of the models internally, allowing developers to create models without getting into the details of the machine learning algorithms. SASI will also include a collection of standard and novel model validation tools such as the k-fold cross validaation method and 66-33\% split model validation method.    
 
 \item { \bf Model Integration}. Using a pre-compiled SASI model at runtime is as simple as extracting the required features from the given computational model and feeding them into the SASI model. The model will then return the optimal solver for the given problem and architecture that can be used to solve the problem. 
\end{itemize}

\subsubsection{SolverSelector: Automatic Solver Selection for Linear Solvers} 
\label{sec:linearsolvers}

The fundamental ideas and workflows proposed for SASI were developed during the project teams ongoing Phase II SBIR project entitled Automated Solver Selection for Nuclear Engineering Simulations (Contract \#DESC0013869). In the follow section we will outline the results of that work, as well as highlight the key differences between that work and the work being proposed in this Phase I proposal.  

The primary objective of the Solver Selection SBIR project was to develop models for automatically selecting the optimal linear solver configuration based on the given computational model and the performance goals of the user. For linear solvers, the computational model is a matrix vector system, $A\mathbf{x} = \mathbf{b}$, hence, the feature set is comprised of structural and physical properties of the matrix. An example of matrix features used includes the number of non zeros, the max number of non zeros per row, the non zero symmetry pattern, the infinity norm and the trace.  

%\begin{table*}[h]
%\centering
%\tiny
%\caption{Linear Solver Feature Set}
%\label{reduced_sets}
%\resizebox{\linewidth}{!}{%
%\begin{tabular}{|c|c|c|}    \hline  
%
%Features                   & Reduced Set 1  & Reduced Set 2 \\ \hline\hline
%Min. Non zeros/row         & X              & X             \\ \hline
%Lower Bandwidth            & X              & X             \\ \hline
%Non Zero Pattern Symmetry   & X              &               \\ \hline
%Infinity Norm              & X              &               \\ \hline
%Column Variance            & X              & X             \\ \hline
%Diagonal Non Zeros         & X              & X             \\ \hline
%Diagonal Average           & X              & X             \\ \hline

%\end{tabular}}
%\end{table*}

%n SolverSelector, feature set reduction was completed manually by applying multiple attribute evaluators with different search methods to select the features that are significant and contribute the most towards the classification process. The SASI framework will automate this process by running each feature set through a rigorous set of attribute evaluators and search methods to produce a set capable of producing a machine learning model with a high level of accuracy per computational cost. (See Section~\ref{workplan-feature-set}

The training data was collected from two sources; the SuiteSparse matrix collection (formally the Florida Sparse matrix collection) and a set of matrices extracted from the MOOSE testing suite. MOOSE is a multiphysics finite element code built on top of libMesh that provides a number of finite element examples across a broad range of application areas \cite{MOOSE}. 

The system $A\mathbf{x} = \mathbf{b}$ was solved multiple times for each matrix in the datasets, each time using a different linear solver configuration. The RHS was a vector of ones and a random initial guess was used when needed. An entry in the training data set was classified as ``good'' or ``bad'' using a binary classification system. Specifically, a solver was classified as ``good'' for a given matrix if the measured value (i.e., CPU time, memory usage) for that solver was within $80\%$ of the best solver for the given matrix. Eighty percent was chosen based on experimentation because it meant a small number of solvers are classified as good for each matrix while not being so restrictive that the runtime models cannot find a good solver. 

%The downside of a classification based machine learning models is that they do not rank the solvers in order of effectiveness. Rather, the model can only be used to predict the performance of a single solver. This means that for any given feature set, there may be multiple solvers classified as ``good''. Moreover, the only way to find the list of ``good'' solvers is to loop through and test every solver configuration available in the database. For linear solvers. we simply returns the first ``good'' solver as the optimal solver, a decision that balances the risk of not getting the very best solver against the costs of testing every solver in the database. The project team has developed a ranking based machine learning algorithm for graph problems; however, the results of those tests are yet to be verified. Those ranking based machine learning models will be integrated into the SASI framework in Phase II. 

\begin{table*}[h]
\centering
\caption{Convergence model accuracy and build time for 10-fold cross validation with SuiteSparse data set.}
\label{RS1_10cv_UF}
\begin{tabular}{|c|c|c|}    \hline  

Method                & Overall accuracy ($\%$)  & Build time \\ \hline\hline
BayesNet              &	96.1                    & 0.11	\\ \hline	    
RF(100)   &           97.0                    & 3.70	\\ \hline      
ADT                              &  93.1	                & 0.70	\\ \hline  
knn(10)                             &	92.0                    & 0.01	\\ \hline	    
J48                           &	95.7                    & 3.70	\\ \hline 	    
\end{tabular}
\end{table*}

Table~\ref{RS1_10cv_UF} shows 10-fold cross validation tests for the machine learning models built using the SuiteSparse matrix collection. The best method, Random forest with 100 trees, classified good solvers as good and bad solvers as bad with an accuracy of 97\% over the 50000 individual entries in the data set.

The main research goal of the SolverSelector project was to create a blueprint through which smart algorithm selection models can built. To that end, one of the success stories of the project has been the creation of a robust, documented and published method for the creation of smart algorithm selection models that has successfully applied to other algorithm classes such as eigensolvers and graph algorithms.   

The development of Solver Selector API, which was built as a means of integrating automatic solver selection in NEAMS tools and is the primary commercial product of the SBIR Phase II award, has been equally successful. The SolverSelector API provides a simple interface through which users can use the linear solver models in advanced numerical tools. In particular, the API has a simple database management system based on Sqlite3, an interface to the Waffles machine learning toolkit, and built in support for gathering linear solver based training data. Currently, the SolverSelector API has also been integrated into PETSc as a standalone KSP solver and can be used in any PETSc based code (MOOSE, libMesh, Fenics, Proteus, etc) through the addition of a copuple of lines of code and two command line parameters.

\subsubsection{The need for a SASI framework} 

RNET and UO have been working on developing, optimizing and implementing the blueprint for linear solver based smart algorithm selection for just over three years. That blueprint has repeatedly proven to be a robust pathway towards creating models for new classes of numerical algorithms; however, it has also proven to be incredibly difficult to implement, even under the guidance of the creators of the approach. As an example, it took a graduate student 9-12 months to develop the first smart algorithm selection model for graph algorithms, with numerous hickups and difficulties along the way. 

While the SolverSelector API does provide some levels of automation (i.e., automated model building using Waffles and integrated database management with Sqlite3), it was not designed for generic numerical algorithms and was never intended to be a general toolkit for building the SASI models. That is to say, the goal of the SolverSelector project was to create and sell the technology (i.e., the SASI models), whereas, the goals of the proposed Phase I project will be to develop and sell the mechanisms for making the technology accessible to the broad numerical simulation community. Currently RNET is in a position to build and sell prebuilt SASI models and custom integration software for any new application, but, the process for developing those manuels is completely manual. Based on the demostrated performance of the models, we feel there will be a large market for a framework that automates the model building process, making smart algorithm selection available in academic and industrial settings where developing a model from scratch would otherwise be unfeasible.  

SASI will be designed to guide the user through the process of building and integrating the models for generic algorithms, providing tools and mechanisms for avoiding the numerous hickups and complications that often arise. This will include tools for verifying the discriminatory properties of the features in the feature set, tools for visualizing how well the dataset covers the feature space, and tools for performing efficient data collection on large scale machines. 

The SASI framework will also look to address one key issue associated with the existing SASI models; the requirement to rebuild the training data set from scratch for each new architecture and for each new algorithm. To that end, the SASI models developed with the proposed toolkit will be designed to facilitate the development of numerical software that automatically adapts to the problem being solved and to the compute environment being used, irrespective of the size (laptop, petascale, exascale, cloud) and type (CPU, GPU, FPGA, etc) of the available resources. The ability to adapt to new architectures and algorithms will be of particular importance on exascale resources, where the cost of running exascale simulations and the expected high demand for the resources during the initial rollout will make it difficult to obtain the exascale compute time required to build accurate SASI models. The SASI framework will leave the definition of "optimal" up to the user, allowing the developers to create simulations that take into account the strengths and weaknesses of the available resources. For example, on exascale resources, the end-user could configure the SASI models to pick algorithms based on minimizing the power consumption or maximizing resilliancy. Likewise, on a cloud based cluster where network speeds can be an issue, simulations could be configured to prioritise algorithms with a high ratio of computational to communication.  
