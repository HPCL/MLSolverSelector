\section{Identification and Significance of the Problem or Opportunity, and Technical Approach}
%% {\em Define the specific technical problem or opportunity addressed by
%%  your application.  Provide enough background information so that the
%%  importance of the problem/opportunity is clear.  Indicate the overall
%%  technical approach to the problem/opportunity and the part that the
%%  proposed research plays in providing needed results.}

RNET Technologies Inc. (RNET) in Dayton, OH and Professor Boyana Norris from the University of Oregon (UO) are responding to the 2019 DOE 
SBIR/STTR (Release 1) Topic 7b: Technologies for Extreme Scale Computing (Software). RNET and UO are proposing are proposing the development of SASI (Smart Algorithm Selection through Inference); a machine learning based toolkit for the run-time optimization of numerical algorithms in terms of performance metrics such as CPU-time, memory consumption, energy usage, and resilience. The SASI framework will be built based on the ideas, workflows and results obtained in the projects team previous work developing machine learning models for optimal algorithm selection in linear solver packages such as PETSc. In addition to this ongoing work, RNET has extensive experience in various aspects of High Performance Computing such as performance 
optimization of numerical softwares and libraries, development of fine-grained power monitoring tools 
for HPC infrastructure, and large scale data analysis tools. Dr. Boyana Norris (UO) has extensive experience 
in enabling technologies for high-performance simulations in computational science and engineering. 
Specifically, her research on performance analysis of scientific codes spans a spectrum of topics, 
including static analysis, runtime performance monitoring, performance database management, postmortem 
performance analysis, and source transformation tools for performance tuning. Therefore, 
this team is well positioned to develop and commercialize the proposed smart algorithm selection package for 
large-scale numerical simulation. 


\subsection{Identification and Significance}
\label{sec:introduction}

In an effort to further improve the nations numerical simulation capabilities, the DOE, academia and other government agencies have invested heavily in the development of efficient algorithms and implementations for solving the broad spectrum of subproblems that arise in numerical simulation.  The result of this investment is a robust collection of software packages designed to solve numerical subproblems across a broad spectrum of applications and compute architectures. One only needs to look at the expansive collection of linear solvers and preconditioners available in PETSc to see the shear number of different implementations and algorithms that have been developed to solve just one, albeit important, class of numerical subproblem.  

While much work has taken place in optimizing the algorithms and implementations for specific problems and architectures, there is no simple governing theory for choosing between the numerous algorithms and configurations. Rather, the optimal method is, in practice, determined by experimentation and numerical folklore~\cite{EijkFuen2010:multistage}. While the experience and expertise provided by the domain scientists in tuning the algorithms cannot be underestimated, it is desirable to have toolkit-enabled functionality that automatically chooses the algorithm that is both appropriate for the computational problem at hand \emph{and} the computer architecture that will be used. Automatic selection of an appropriate algorithm and configuration can lead to benefits such as reduced memory requirement, lower execution time, fewer synchronization points in a parallel computation and so forth. 

As an example, consider the large sparse linear systems that arise when solving a nonlinear partial differential equation. While one could take a guess as to the best linear solver based on coarse grained problem characteristics and experience, it is all but impossible to robustly determine the best solver in all cases. In fact, it has been shown that there is no uniformly optimal linear solver for general matrix-vector systems \cite{TODO}. Moreover, Eller \cite{Eller2012} showed that as the simulation progresses and the linear systems produced by the simulation changes, the best preconditioned iterative solver to solve each linear system can also change. Similarly, in applications with dynamic mesh adaptation, runtime changes in the structure of the mesh (both physical and geometrical) have also been shown to lead to changes in the linear system and hence, the optimal linear solver. The memory constraints of the solver can also change according to the code implementation or architecture of the machine, highlighting the fact that the compute architecture also plays a huge role in the performance of different algorithms.

In other words, for optimal performance, the choice of linear solver should not be a static, a-priori design parameter, but rather, a dynamic, architecture dependent decision made based on the properties of the problem and the performance goals of the user. The same argument holds true for all numerical subproblems solvers, including eigensolvers, nonlinear solvers, graph algorithms and nonlinear optimization routines.      

To address this need, RNET and the University of Oregon are proposing the Smart Algorithm selection through Inference (SASI) framework. The idea for SASI was born out of the ideas, lessons and workflows developed during the project teams previous work on automatic solver selection for linear solvers, eigen-solvers and graph algorithms. That previous work has continuously shown that SASI style performance models were capable of repeatedly predicting, with an extremely high accuracy (i.e., 98\%+), the performance of numerical algorithms and solver configurations across a wide range of applications and domains.

Overall, SASI will be a generic toolkit that guides the user through the process of building and using smart algorithm selection models in modern numerical simulations. While SASI will be portable to any compute architecture (GPU, Cloud FPGA, etc.), the framework developed in this project will target the automatic optimization of numerical algorithms designed for execution on multi-node distributed memory systems ranging on user owned clusters up to leadership class exa-scale machines. SASI provides developers with a mechanism for allowing the problem and architecture to determine the optimal solver, removing the need for time consuming parameter tuning. Ultimately, SASI will allow developers to ship tools that run optimally on any architecture, speeding up development across the entire design pipeline; from early stage testing on single desktops through to final exascale production runs.

\subsubsection{Smart Algorithm Selection Through Inference }

The SASI framework will guide the user through the process of training, building an using machine learning models targeted at algorithm optimization, automating the process where ever possible. The process of building a SASI model can be split into four distinct components; feature determination, data collection, model building and efficient software integration, each of which is rife with intricate details and nuances that must be addressed to ensure optimal performance in the final simulation:

\begin{itemize}
 \item {\bf Feature Determination:} Perhaps the most difficult component of building a SASI model is that of feature determination and evaluation. In machine learning terminology, a feature is any obtainable metric, measurement or observation that can be made about the target phenomenon. Determination of an informative, discriminating and independent set of features is a crucial step in the creation of unbiased and useful classification and regression based machine learning algorithms such as those used in SASI. In SASI, a feature represents a computable metric of the computational model (i.e, the linear system in a linear solver or the graph in graph solvers), that can be used to indicate or predict the performance of an algorithm when applied to a specific problem and on a specific architecture. 
 world application (see Section~\ref{sec:linearsolvers}. 
 
 \item Data Collection: In the context of SASI, training data takes the form of a set of measurements obtained by applying the different algorithms and configurations to a diverse set of sample problems. For linear solvers, this means solving a large number of matrix-vector systems using a wide variety of linear solvers, preconditioners and input parameters. Likewise, for graph algorithms, the training data set consists of measurements and features obtained by solving a collection of graph problems using a range of graph algorithms. Data collection is the most expensive step in SASI workflow; however, once an initial dataset has been built, it can be reused and extended continuously, quickly making up for the initial cost. 
 
 \item Model selection and Validation. The third component of the SASI workflow involves choosing a machine learning algorithm, building the model, and testing the result. Previous results have shown that simple machine learning algorithms such a a single decision tree or a Random forest can be used to create cheap and highly accurate machine learning models. For instance, the decision tree based J48 algorithm was the most accurate algorithm for the linear solver based SASI models outlined in Section~\ref{sec:linearsolvers}.  
 
 \item Model Integration. The final component of the SASI workflow is software integration. Integrating SASI into existing simulations is somewhat software dependent. Using a pre-compiled SASI model at runtime is as simple as extracting the required features from the given computational model and feeding them into the SASI model. The model will then return a the optimal solver for the given problem and architecture that can be used to solve the problem.  

\end{itemize}

\subsubsection{SolverSelector: Automatic Solver Selection for Linear Solvers} 

The fundamental ideas and workflows proposed for SASI were developed primarily during the project teams ongoing Phase II SBIR project entitled Automated Solver Selection for Nuclear Engineering Simulations (Contract \#DESC0013869). In that work, the project team developed a robust framework for building and integrating machine learning models for smart algorithm selection of the linear solvers used in nuclear engineering. In the follow section we will outline the results of that work, as well as highlight the key differences between that work and the work being proposed in this Phase I proposal.  
ly. 

The primary objective of the Solver Selection SBIR project was to develop models for automatically selecting the optimal linear solver configuration based on the given computational model and the performance goals of the user. 

The first step in creating a SASI model is to determine the features. Recall that the features are a set of metrics that, in some way or another, infer the performance of the given algorithm on the given computational model. For linear solvers, the computational model is a matrix vector system, $A\mathbf{x} = \mathbf{b}$; hence, the feature set is comprised of structural and physical properties of the matrix. An example of matrix features used includes the number of non zeros, the max number of non zeros per row, the non zero symmetry pattern, the infinity norm and the trace.  

%\begin{table*}[h]
%\centering
%\tiny
%\caption{Linear Solver Feature Set}
%\label{reduced_sets}
%\resizebox{\linewidth}{!}{%
%\begin{tabular}{|c|c|c|}    \hline  
%
%Features                   & Reduced Set 1  & Reduced Set 2 \\ \hline\hline
%Min. Non zeros/row         & X              & X             \\ \hline
%Lower Bandwidth            & X              & X             \\ \hline
%Non Zero Pattern Symmetry   & X              &               \\ \hline
%Infinity Norm              & X              &               \\ \hline
%Column Variance            & X              & X             \\ \hline
%Diagonal Non Zeros         & X              & X             \\ \hline
%Diagonal Average           & X              & X             \\ \hline

%\end{tabular}}
%\end{table*}

%n SolverSelector, feature set reduction was completed manually by applying multiple attribute evaluators with different search methods to select the features that are significant and contribute the most towards the classification process. The SASI framework will automate this process by running each feature set through a rigorous set of attribute evaluators and search methods to produce a set capable of producing a machine learning model with a high level of accuracy per computational cost. (See Section~\ref{workplan-feature-set}

The second step in model building is to collect the data. For linear solvers, the training data was collected from two sources; the SuiteSparse matrix collection (formally the Florida Sparse matrix collection) and a set of matrices extracted from the MOOSE testing suite by scaling the appropriate input parameters. MOOSE is a multiphysics finite element code built on top of libMesh that provides a number of finite element examples across a broad range of application areas \cite{TODO-MOOSE}. 

Training data was obtained by solving the system $A\mathbf{x} = \mathbf{b}$ for each of the matrices in the data sets using a range of linear solver configurations available in PETSc \cite{PETSc}. In each case the RHS was a vector of ones and a random initial guess was used when needed. A solver in the training data set was classified as ``good'' or ``bad'' using a binary classification system. Specifically, a solver was classified as ``good'' for a given matrix if the measured value (i.e., CPU time, memory usage) for that solver is within $80\%$ of the best solver for the given matrix. Eighty percent was chosen based on experimentation because it meant a small number of solvers are classified as good for each matrix. 

%The downside of a classification based machine learning models is that they do not rank the solvers in order of effectiveness. Rather, the model can only be used to predict the performance of a single solver. This means that for any given feature set, there may be multiple solvers classified as ``good''. Moreover, the only way to find the list of ``good'' solvers is to loop through and test every solver configuration available in the database. For linear solvers. we simply returns the first ``good'' solver as the optimal solver, a decision that balances the risk of not getting the very best solver against the costs of testing every solver in the database. The project team has developed a ranking based machine learning algorithm for graph problems; however, the results of those tests are yet to be verified. Those ranking based machine learning models will be integrated into the SASI framework in Phase II. 

Table~\ref{TODO} shows 10-fold cross validation and 66-33\% split validation tests for the machine learning models built using the Florida Sparse matrix collection. Overall, the models obtained for linear solvers where capable of classified ``good'' solvers as ``good'' with an accuracy of up to $TODO\%$ in both the 10-fold cross validation and 33-66\% split validation tests. Similar results were found for the MOOSE data set and for other algorithms subclasses such as eigensolvers and graph algorithms. 

The product generated from the Phase II prject is called SolverSelector. SolverSelector is designed to act as an interface between linear solver packages such as PETSc and machine learning toolkits such as TensorFlow. For that reason, SolverSelector was designed to mimic the behaviour of a linear solver. As such, integration of a automatic solver selection into an existing solver package is as simple as implementing the interface and calling SolverSelectors solve function whenever a linear solve is required. SolverSelector has implemented in PETSc as a standalone KSP solver, and has been used successfully used inside PETSc based simulation software such as MOOSE, LibMesh and PROTEUS.

The main research goal of the SolverSelector project was to create a blueprint through which smart algorithm selection models can be built for individual numerical algorithms. To that end, one of the success stories of the project has been the creation of a robust, documented and published method for the creation of smart algorithm selection models that has been applied to other algorithm classes such as eigensolvers and graph algorithms.   

The development of Solver Selector API, which was built as a means of integrating automatic solver selection in NEAMS tools and is the primary commercial product of the SBIR phase II award, has been equally successful. The SolverSelector API provides a simple interface through which users can use the linear solver models in advanced numerical tools. In particular, the API has a simple database management system based on Sqlite3, an interface to the Waffles machine learning toolkit, and built in support for gathering linear solver based training data. Moreover, included with the API is an interface for integration with PETSc, wherein, the API is integrated as a standalone KSP solver. 

\subsubsection{The SASI framework} 

RNET and UO have been working on developing, optimizing and implementing the blueprint for smart algorithm selection in linear solvers for just over three years. That blueprint has proven time and time again to be a robust pathway towards creating models for new classes of numerical algorithms; however, it has also proven to be incredibly difficult to implement, even under the guidance of the creators of the approach. As an example, it took a graduate student 9-12 months to develop the first smart algorithm selection model for graph algorithms, with numerous hickups and difficulties along the way. 

While the SolverSelector API does provide some levels of automation (i.e., automated model building using Waffles and integrated database management with Sqlite3), it was not designed for generic numerical algorithms and was never intended to be a general toolkit for building the SASI models. Rather it was designed as a plugin for linear solver packages to include smart algorithm selection. Similarly, service contracts offered by RNET for supporting new algorithms and applications are designed such that RNET will design and build the entire system, from determining the features through  to gathering the data, building the model and performing software integration; this is simply no way that an industry based company would have the time and resources to otherwise implement a new model. 

The SASI framework will be a sophisticated machine learning toolkit designed to make SASI integration feasible for developers across the broad spectrum of numerical simulation algorithms and applications. Overall, the models built using this approach will ensure the efficient usage of large-scale resources (both in terms of time and energy). 

The defining design guidelines of SASI will be simplicity and generality. That is to say, SASI will be designed to guide the user through the process of building and integrating the models for generic algorithms, providing tools and mechanisms for avoiding the numerous hickups and complications that often arise. This will include tools for verifying the discriminatory properties of the features in the feature set, tools for visualizing how well the dataset covers the feature space, and tools for performing efficient data collection on large scale machines. 

One of the key issues associated with smart algorithm selection models,  (and machine learning in general) is that the training data must come from the same domain for which it will be used. That is to say, it is not easy to reuse data from one domain when transitioning to a new domain. This is a particular problem with the smart algorithm selection models because the cost of data collection is so large ( i.e, generating the MOOSE training data set required the completion of over 150000 linear solves) and because the data set must be regenerated on each new architecture. 

Because SASI will be designed for generic algorithms, addressing these issues will be a key area of research for the Phase I and Phase II projects. Whereas the dataset of the SolverSelector models needs to be regenerated on each new architecture, SASI will be designed to facilitate the development of numerical software that automatically adapts to the problem being solved and to the compute environment being used, irrespective of the size (laptop, petascale, exascale, cloud) and type (CPU, GPU, FPGA, etc) of the available resources, with applications across all areas of numerical simulation. 

An additional benefit of the proposed adaptive models will be that they will equip SASI models with a high level of predictive capabilities. That is to say, users will be able to predict the performance of a solver or algorithm on a new architecture based solely on previous results. The ability to adapt to new architectures and algorithms and to make use of previous simulation results will be of particular importance on exascale resources, where the cost of running exascale simulations combined with the expected high demand for the resources during the initial resources will make it difficult to obtain the exascale compute time required to build accurate SASI models. 

In summary, SASI will be a machine learing toolkit designed to optimize the process of building a smart algorithm selection model for any numerical algorithm capable of determining the optimal algorithm and configuation for the given problem and the given compute architecture. To encourage uptake of SASI in numerical simulation, the core SASI framework will be released as Open Source software, with the commercialization plan being to offer support, training, extension and/or integration contracts to government agencies and private companies looking for automated optimization of existing simulation toolkits.  Open source, contract based commercialization is quickly becoming the norm in the numerical simulation community, primarily because it appeals to a user-base that almost exclusively deals in free open source software. In return, the results, papers and simulation toolkits released utilizing SASI models act as free, no-cost marketing for SASI, further driving uptake of SASI while also increasing the likelihood of obtaining new service contracts. In addition to this, RNET will also maintain a database of simulation based training data that will be made available at an additional cost to any users looking for data to kickstart a new SASI model. 
