
\section{Related Work}
\label{related}

%%% Put in something about my thesis to say we have experience with the solver. 
RNET has past and current experience in several SBIR/STTR projects on modeling and simulation, high performance computing, and large data formats. The proposed project is an extension and generalization of RNETs Phase II automated linear solver selection project as outlined in Section~\ref{TODO}. Some additional related projects completed by RNET are briefly described below. 

\subsection{Verification and Validation Toolkit for Nuclear Engineering Simulations} 
 Testing and debugging is a major component of software development; in fact, nearly 50\% of all code development consists of debugging existing code \cite{britton_debugging}. For numerical simulation codes being used to design real world nuclear reactors, erroneous simulations can result in design errors that can be extremely expensive to fix, damage the environment, and ultimately result in loss of human life. To aid in the verification and validation of numerical simulations, RNET is developing the V\&V toolkit. The toolkit will offer a complete set of verification and validation routines designed to rigorously test every aspect of numerical simulation. 

%\subsection{VERA-WorkBench: A unified multi-physics interface for the VERA Suite of Tools}
%Nuclear engineering simulations typically involve a multi-scale setup to resolve phenomena
%at various levels of fidelity. There is a need for a multi-physics environment (like an 
%Integrated Development Environment (IDE)) that allows users to setup, install, execute codes
%and post-process results. Such a framework would also support coupling between various codes and
%would enable novice users to adopt VERA codes quickly. The VERA-Workbench is a Python-based platform that will
%offer an IDE like environment for the various VERA tools to be used. The workbench is slated to support
%processing of VERA input files using the VERAin processor, job launch and execution using one of the VERA-CS 
%tools and develop APIs to carry out visualizations and post-processing of output from the numerical codes. 
%The proposed tools will significantly encourage the adoption of VERA-CS tools by removing the hassle in
%compilation and environment setup. Additionally the proposed platform will interact with local and cloud-based 
%clusters and readily allow access to latest developments in hardware to better improve simulation performance.

\subsection{Cloud-based Scientific Workbench for Nuclear Reactor Simulation Life Cycle Management}
The predictive modeling approaches and softwares being continually developed and updated by the DOE nuclear engineering scientists (under programs such as NEAMS, CASL, RISMC etc.) need to be efficiently transferred to the nuclear science and engineering community. An advanced workflow management workbench is required to allow efficient usage from small and large business and research groups. The workbench must manage inputs decks, simulation execution (on a local machine, a High Performance Compute cluster, or a Cloud cluster), intermediate results, final results and visualizations, and provenance of the tools and settings. CloudBench is a hosted simulation environment for large scale numeric simulations. CloudBench will augment existing simulation, Integrated Development Environment, and workbench tools being developed by the DOE and industry. It offers a complete set of simulation management features not available in open tools: sharing of configurations, simulation output, and provenance on a per simulation or per project basis; multi-simulation provenance history to allow simulations to be reconstructed, verified, or extended; and remote access to simulation tools installed on Cloud and HPC resources. The portal enables easy adoption of government codes. 

\subsection{Scaling the PETSc Numerical Library to Petascale Architectures}

RNET has developed an extended version of the numerical library PETSc \cite{Lowell1} in
collaboration with Ohio State University and Argonne National Lab. PETSc is an MPI-based numerical library of
linear and nonlinear solvers that is widely used in a variety of scientific domains. With the
emergence of multicore processors and heterogeneous accelerators as the building blocks of
parallel systems, it is essential to restructure the PETSc code to effectively exploit multi-level
parallelism. Changes to the underlying PETSc data structures are required to leverage the multicore
nodes and GPGPUs being added to the ``cluster architectures''.

This project was funded by Department of Energy under the STTR program from August 2010 (Contract Number DE-SC0002434) 
to May 2013. Dr. P. Sadayappan (OSU) and Dr. Boyana Norris (ANL) have played a key role in this effort by serving as 
technical advisors. As part of the project, the team has investigated ways for the PETSc library to fully utilize the 
computing power of future Petascale computers. Novel sparse matrix types,  vector types, and preconditioning techniques 
that are conducive for GPU processing and SIMD parallelization have been integrated into the PETSc library. The matrix 
vector operations have been optimized for specific architectures and GPUs by 
utilizing the autotuning tools.


\subsection{A Map-Reduce Like Data-Intensive Processing Framework for Native Data Storage}

RNET is currently under a DOE Phase II STTR contract for developing a MapReduce-like data-intensive processing framework 
for native data storage (Contract\#: DE-SC0011312). The Ohio State University (OSU) is a collaborator on this STTR project. 
MapReduce is a very popular data analytic framework that is widely used in both industry and scientific research. Despite 
t%he popularity of MapReduce, there are several obstacles to applying it for developing some commercial and scientific data 
analysis applications.

This project is developing a Native data format MapREDuce-like framework, iNFORMER, based on SciMate architecture. The 
framework allows MapReduce-like applications to be executed over data stored in a native data format, without first loading 
the data into the framework. This addresses a major limitation of existing MapReduce-like implementations that require the 
data to be loaded into specialized file systems, e.g., the Hadoop Distributed File System (HDFS). The overheads and additional
data management process.


