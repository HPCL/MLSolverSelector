\section{Work Plan} 
\label{sec:approach}
%% {\em Provide an explicit, detailed description of the Phase I research
%%   approach and work to be performed.  Indicate what will be done, by
%%   whom (small business, subcontractors, research institution, or
%%   consultants), where it will be done, and how the work will be
%%   carried out.  If applicant is making a commercial or in-kind
%%   contribution to the project, please describe in detail here.  The
%%   Phase I effort should attempt to determine the technical feasibility
%%   of the proposed concept which, if successful, would provide a firm
%%   basis for the Phase II grant application.

%%   Relate the work plan to the objectives of the proposed project.
%%   Discuss the methods planned to achieve each objective or task
%%   explicitly and in detail.  This section should be a substantial
%%   portion of the total grant application.} 

Development of the SASI framework will be composed of two connected but distinct components; the development of numerous tools, analysis routines and machine learning algorithms required to build, test and validate  the proposed SASI models, and the development of the core framework for gluing together those tools. In what follows, we outline the approach that will be taken to achieve the overall objectives of the proposed project. 

\subsection{ Model, Data and Feature Set Verification and Analysis }
\label{sec:modelvar}
The number one requirement for the development of an accurate machine learning model is a robust, discriminating and informative set of training data, features and classification metrics. Speaking from experience, it is all to easy to create a machine learning model that performs wonderfully well in testing and validation tests, but that has little to no predictive capabilities in real-world applications. Based on previous research, the primary causes for poor performance in SASI models are; (1) a feature set with poor discriminating capabilities, (2) a data set that does not adequately cover the parameter space of the given feature set and (3) a feature set that is too expensive for use in run-time applications. To that end, the SASI framework will include a range of tools dedicated to assessing the validity of the given model, feature set and data set. In particluar, the Phase I effort will focus on the development of the following two tools: 

\subsubsection{Feature Sampling}  The process of determining which features are appropriate for a given algorithm is highly problem dependent and best left up to a domain expert. However, once the features have been determined, there are several existing methods for assessing the validity of the feature set and for optimizing it for use in SASI models. One example of this is known as feature sampling, whereby we remove features from the feature set that do not possess enough variability to be of any statistical importance. For SASI, removing irrelevant features is incredibly important because it reduces the runtime cost associated with feature extraction, thereby increasing the potential performance benefits of using SASI models. 

In our previous work, feature sampling was completed manually using the many feature evaluation tools available in the Weka toolkit, including the CfsSubsetEval with BestFirst search method, Gain Ratio, Info Gain and the Principle Components evaluators with Ranker search method. In this Phase I effort we will develop a tool for the automation of this process. This will include the development of an interface for interacting with the Java based weka toolkit programatically, as well as mechanisms for automatically verifying the results. 

This automated feature sampling tool will also include an optimization algorithm for optimizing the feature set based on maximizing the accuracy of the model and minimizing the computational costs of feature extraction. Feature set optimization is an offline task and, in previous research, full feature sets have been composed of around 30 discrete features; hence, a simple brute force search of the parameter space will likely be efficient enough to complete this optimization. If the efficiency of the method becomes an issue, more complex optimization methods will be investigated. 
  
\subsubsection{Database Management and Sanitation} Management of the large datasets required in machine learning models can be a complex and time consuming task. In addition to simply parsing the data into the machine learning model, one must also be concerned with cleaning, sanitizing and validating the datasets. To ensure this is an easy task, SASI will include a generalized version of the sqlite3 database management system developed for SolverSelector. To address database sanitation, the project team will develop simple interfaces for performing data orientated tasks such as detecting classification conflicts, replacing or removing missing data, rescaling data to fix prescribed ranges and automatic discretization of data into classifications. These are all standard, albeit time consuming, machine learning tasks that are well suited for automation. 

\todo{ @Boyana: Are there any machine learning based tests out there for testing how well the data set covers the parameter space of the feature set. Or any other tools you can think of. I am at a loss. The next paragraph might very well be giberish } 
A narrow data set is a major reason for poor performance of machine learning models. However, because machine learning models are generally tested using the same data set from which they are built, it can be very difficult to assess the variability of the dataset. The Phase I effort will look to address this issue by developing a tool for visualizing and quantifying the variability of a given dataset. SASI feature sets can have up to 30 features,This will be achieved by utlizing a mixture of statistical metrics (e.g., variance, parsing the data-set and determining areas of the feature set parameter space for which there is limited data. This data will then be presented visually as an intuitive database coverage map. This will allow users and developers to quickly determine areas where additional training data is required. In turn, this will allow them to focus the data collection efforts on applications that will have a real statistical impact on the performance of the model, rather than repeatedly gathering data covering the same area of the feature space. 


\subsection{ Re-purposing training data for use on other applications and architectures }
\label{sec:reuse}
In previous work, the machine learning models and training data has been architecture specific. The consequence of this is that the entire training set must be recomputed for each new architecture, a task that is extremely expensive and time consuming. The situation is similar when applying SASI to a new model because the results from different application areas cannot be used in any way in the new application. This means that, even in cases where the computational model has core components that are similar ( i.e., linear solvers and eigensolvers), the user must create an entirely new algorithm specific data set. Data collection is the most expensive component of the SASI workflow, hence, maximizing data reuse will go a long way towards increasing the appeal of the methods. 

To that end, the Phase I effort will involve a in depth investigation into methods for using data obtained from one architecture and algorithmic class to infer informed predictions on a different architecture or algorithmic class.  

The approach taken to address architecture differences will be to develop models whereby the characteristics of the architecture (i.e., the processor speed, available memory, cores-per-node, network speed, etc) are used as features in the machine learning model. The result will be a model that can, for example, predict that an algorithm will perform poorly on a new architecture because the memory allocations are to small or because the network is to slow. To test this, the project team will generate the linear solver training data set on a range of different architectures and use it to build a single model whereby the linear solver features as well as the machines internal specifications are used as the features. 

To futher enable efficient reuse of data we will also investigate the development of Baysian transfer learning methods. Transfer learning leads to models capable of simultaneously learning from different source domains, making it possible to transfer relevant knowledge from domains with plenty of labeled data to new domains where training data is limited. A good analogy for transfer based learning is that of learning a new programming language, whereby the transfer of core skills from previous languages acts to reduce the learning curve associated with the new language. In the context of SASI, this means using data from a class of algorithms (such as linear solvers) and architecture,  to improve prediction in a similar algorithm class or on another architecture. In Phase I we will use existing data for linear solvers, eigensolvers and graph algorithms to build and test models using several classes of Baysian transfer methods. In particular, we will look at developing a model whereby the learing solvers dataset is used to kickstart the model for eigensolvers. 
\todo{ We discussed these Baysian methods on the phone last week, and this was my best guess at what that might look like. But, I am a lot thin on the details. Any ideas? }


\subsection{The SASI API} 
Gluing all these tools as algorithms together will be the primary SASI API. Completion of this interface will require a large literature review to asses the best methods for developing an interface that is applicable to generic numerical algorithms. Currently, the idea is that the interface will require the user to specify a list of functions for tasks that must be completed to utilize the various tools given inside SASI. All interaction between the domain scientist and the internal SASI tools will go through this interface, with every effort being made to minimize the required number of function definitions. If a function definition is not provided, any tools using that method will be unavailable, but other methods will still be accessible. A similar interface has proven to be very succssesful in the SolverSelector API. Work on documentation and examples will be completed throughout the development of the interface as each new feature is added. 
 
\section{Performance Schedule and Task Plan}
\label{sec:taskplan}

The goal of the Phase I effort will be to provide the reviewers with a clear idea 
of the scientific and commercial potential of SASI. The research and development topics 
described in Section \ref{sec:approach} will be addressed by the tasks described in the remainder of this section. 
Figure~\ref{fig:tasks} summarizes, at a high level, the dependencies among tasks 
and approximate anticipated task durations. The project duration is roughly 
divided into 1 month blocks and is 12 months overall, with Phase II proposals being due at the 10 month mark. The final two months (denoted admin in the task-dependencies table) will be dedicated to wrapping up the project, finalizing the documentation, writing the final 
report and preparing for a Phase II. 

\begin{figure}
\centering
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|}
   \hline 
   \multirow{2}{*}{ Time (Months): } & \multicolumn{12}{|c|}{ Phase I } \\
   \cline{2-12} 
   & 1 & 2 & 3 & 4 & 5 & 6 & 7 &8 & 9 & 10 & 11 & 12 \\
   \hline 
   Task 1 & X & X & X  & X &  X & X & & & & & &  \\
   \hline
   Task 2 & & & & & & X& X & X & X &X & &   \\
  \hline
   Task 3 &X &X &X & X & X & X & X & X & X & X & &\\
  \hline
   Task 4 & & & &  &  &  & X & X & X & X & & \\
  \hline
   Admin & & & &  &  &  &  & & &  & X & X\\
  \hline

\end{tabular}
\caption{Overview of task dependencies and time-line.}
\label{fig:tasks}
\end{figure}


RNET would like to present the project ideas and research plan to the DOE 
Program Manager and other interested scientists interested in performance tuning
through smart algorithm selection. This meeting will 
be scheduled soon after the Phase I contract is awarded. The Kickoff meeting 
will coincide with the Phase I SBIR PI meeting being hosted by the DOE. The 
meeting can be hosted at RNET, a DOE site suggested by the Program Manager or 
via a teleconference. RNET will submit a final report and present the report 
details along with a Phase II work plan to the DOE program manager and other 
interested scientists.

%\begin{figure}
%\centering
%\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
%   \hline 
%   \multirow{2}{*}{ Time (Months): } & \multicolumn{9}{|c|}{ Phase I } \\
%   \cline{2-10} 
%   & 1 & 2 & 3 & 4 & 5 & 6 & 7 &8 & 9 \\
%   \hline 
%   \bcolumn{Task 1} & X & X & X & X & X  &   &   &   & \\
%   \hline
%   \bcolumn{Task 2} &   &  &  & X & X & X & X  & X  &  \\
%  \hline
%   \bcolumn{Task 3} &   &   &   &  &  & X & X &  X & X \\
%   \hline
%
%\end{tabular}
%\caption{Overview of task dependencies and time-line.}
%\label{fig:tasks}
%\end{figure}

\newcounter{taskCount}
\setcounter{taskCount}{0}

\refstepcounter{taskCount}\label{task:1}
\subsection{Task \ref{task:1}: Protype the core SASI framework}
In this task, the project team will begin the process of developing the core SASI framework. As discussed above, the framework
will be developed based on the results of a comprehensive literature review focusing on the methods 
and interfaces used to solve a variety of subproblems in many of the more prominent solver packages.
At the end of this task, the software interfaces and indirections for building and integration of SASI models 
will be set in stone with all remaining work simply adding functionality. A comprehensive user manual for SASI and
the features therein will also be developed as part of this task. 

RNET will take the lead on this task, with UO providing assistance and insight when needed. 

\refstepcounter{taskCount}\label{task:2}
\subsection{Task \ref{task:2}: Model, Data and Feature set Analysis tools. }
In this task, the project team will develop the tools required for verifying the data set, feature set and model as described in Section~\ref{sec:modelvar}. Initial testing of these tools will be completed using the datasets obtained for linear solvers as part of the SolverSelector project. This data provides a good baseline for testing as it has been shown to be effective in informing an accurate SASI based model for linear solvers. The tools developed in this task will also be used in task~\ref{task:4} to verify the suitability of the graph algorithm data. 

RNET will take the lead on this task, with UO providing assistance and insight when needed. 


\refstepcounter{taskCount}\label{task:3}
\subsection{Task \ref{task:3}: Efficient Reuse of Data on new Architectures and Applications.}
In this task, the project team will prototype the approaches for the re-purposing training data. As outlined in Section~\ref{sec:reuse}, this will include an investigation into using data obtained from similar algorithm classes and Baysian methods to kick-start models for new algorithms. To test these methods, RNET and UO will attempt to kick-start a graph algorithm model utilizing training data obtained from linear solvers and applying it to eigensolvers. The Baysian transfer models will be considered successful if, at the end of the task, we can use data obtained from linear solvers to improve the accuracy of models for eigensolvers when only a small subset of the data for eigensolvers is used. 
Additionally, the project team will develop models that use the machines internal capabilities as features in the overall feature set. This will involve collecting training data on a range of machines with various levels of computing prowess. The architecture transfer models developed in this task will be considered successful if, at the end of the project, we can predict the performance of an algorithm for a given matrix on a new architecture with an accuracy of over 90\%. This task constitutes the major novel scientific contribution of the proposed project and is expected to form the bulk of the Phase I effort. 

RNET and UO will collaborate closley on this task. 

\refstepcounter{taskCount}\label{task:4}
\subsection{Task \ref{task:4}: Demonstration of SASI using Graph Algorithms }
\todo{ @Boyana, does this seem like a good choice for a demonstration ( and do you think we even need one at this point ) }
In this task, RNET and UO will demonstrate the capabilities of the SASI framework by applying 
it to graph algorithms. The solver team has tested smart algorithm selection for graph algorithms 
with good results; however, this was a completely manual process and no SolverSelector like framework was ever built. Using 
the SASI framework, the project team will recreate the graph-algorithm models, leading to the development of a SolverSelector
like API. This will allow the project team to create an informative and instructive proof of concept as to the capabilities 
of the SASI framework. 

RNET will take the lead on this task, with UO assisting with data collection and integration. 
