\section{Work Plan} 
\label{sec:approach}
%% {\em Provide an explicit, detailed description of the Phase I research
%%   approach and work to be performed.  Indicate what will be done, by
%%   whom (small business, subcontractors, research institution, or
%%   consultants), where it will be done, and how the work will be
%%   carried out.  If applicant is making a commercial or in-kind
%%   contribution to the project, please describe in detail here.  The
%%   Phase I effort should attempt to determine the technical feasibility
%%   of the proposed concept which, if successful, would provide a firm
%%   basis for the Phase II grant application.

%%   Relate the work plan to the objectives of the proposed project.
%%   Discuss the methods planned to achieve each objective or task
%%   explicitly and in detail.  This section should be a substantial
%%   portion of the total grant application.} 

Development of the SASI framework will be composed of two connected but distinct components; the development of numerous tools, analysis routines and machine learning algorithms required to build, test and validate  the proposed SASI models, and the development of the core framework for gluing together those tools. In what follows, we outline the approach that will be taken to achieve the overall objectives of the proposed project. 

\subsection{ Model, Data and Feature Set Verification and Analysis }

The number one requirement for the development of an accurate machine learning model is a robust, discriminating and informative set of training data, features and classification metrics. Speaking from experience, it is all to easy to create a machine learning model that performs wonderfully well in testing and validation tests, but that has little to no predictive capabilities in real-world applications. Based on previous research, the primary causes of poor performance in SASI models can most often be attributed to; (1) a feature set with poor discriminating capabilities, (2) a data set that does not adequately cover the parameter space of the given feature set and (3) a feature set that is too expensive for use in run-time applications. To that end, the SASI framework will include a range of tools dedicated to assessing the validity of the given model. The Phase I effort will focus on the development of two of those tools, each of which is described below: 

\subsubsection{Feature Sampling} A key component of any machine learning model is an accurate, informative and discriminating feature set capable of inferring knowledge about the given phenomenon. The process of determining which features are appropriate for a given algorithm is highly problem dependent and best left up to a domain expert. However, once the features have been determined, there are several existing methods for assessing the validity of the feature set and for optimizing it for use in SASI models. One example of this is known as feature Sampling, whereby we remove features from the feature set that do not possess enough variability to be of any statistical importance. For SASI, removing irrelevant features is incredibly important because it reduces the runtime cost associated with feature extraction, thereby increasing the potential performance benefits of using SASI models. 

In our previous work, feature sampling was completed manually using the many feature evaluation tools available in the Weka toolkit, including the CfsSubsetEval with BestFirst search method, Gain Ratio, Info Gain and the Principle Components evaluators with Ranker search method. In this Phase I effort we will develop a tool for the automation of this process. This will include the development of an interface for interacting with the Java based weka toolkit programatically, as well as mechanisms for automatically verifying the results. 

This automated feature sampling tool will also include an optimization algorithm for optimizing the feature set based on maximizing the accuracy while minimizing the computational costs of feature extraction. Feature set optimization is an offline task and, in previous research, full feature sets have been composed of up to 30 discrete features; hence, a simple brute force search of the parameter space will likely be used to complete this optimization. If the efficiency of the method becomes an issue, more complex optimization methods will be investigated. 
  
\subsubsection{Database Management, Sanitation and Validation} Management of the large datasets required in machine learning models can be a complex and time consuming task. In addition to simply parsing the data into the machine learning model, one must also be concerned with cleaning, sanitizing and validating the datasets. To ensure this is an easy task, SASI will include a database management component built using Sqlite3. 

For the most part, the core framework for the sqlite3 database management system developed for SolverSelector is ready for use is SASI; however, there is some requirement for generalizing the schema by which data is stored to the database. As such, the Phase I effort will focus on developing the database sanitation and validation tools. 

To address database sanitation, the project team will develop simple interfaces for performing standard machine learing task such as detecting classification conflicts, replacing or removing missing data, rescaling data to fix prescribed ranges and automatic discretization of data into classifications. These are all standard, albeit time consuming, machine learning tasks that are well suited for automation. 

SASI will also include a variety of tests for assessing data validity supported by a wide range of statistical metrics. For Phase I, the project team will focus on fixing issues associated with data variability. A narrow data set is the number one reason for poor performance of machine learning models. However, because machine learning models are generally tested using the same data set from which they are built, it can be very difficult to assess the variability of the method. The Phase I effort will look to address this issue by developing a tool for visualizing and quantifying the variability of a given dataset. This will be achieved by parsing the data-set and determining areas of the feature set parameter space for which there is limited data. This data will then be presented visually as an intuitive database coverage map. This will allow users and developers to quickly determine areas where additional training data is required. In turn, this will allow them to focus the data collection efforts on applications that will have a real statistical impact on the performance of the model, rather than repeatedly gathering data covering the same area of the feature space. 


\subsection{ Re-purposing training data for use on other applications and architectures }

In previous work, the machine learning models and training data has been architecture specific. The consequence of this is that the entire training set must be recomputed for each new architecture, a task that is extremely expensive and time consuming. The situation is even worse when applying SASI to a new model because the results from application area cannot be used in any way in the new application. This means that, even in cases where the computational model has core components that are similar ( i.e., linear solvers and eigensolvers), we cannot use the results or data from the old domain in the new domain; instead requiring the user to build a new set of training applications from scratch.  

This is the key research issue the project team will need to address for SASI to be a success. To that end, the phase I effort will involve a concerned investigation into methods for using data obtained from data on other architectures and/or from other algorithmic classes to infer informed predictions of algorithm performance on architectures and/or algorithmic classes for which limited training data is available. 

The approach taken to address architecture differences will be to develop models whereby the characteristics of the architecture (i.e., the processor speed, available memory, cores-per-node, network speed, etc) are used as features in the machine learning model in addition to the computational model based features. This will allow for the creation of models whereby the features of the machine are incorporated into the prediction, while also adding little to no computational overhead to the overall feature extraction process. The result will be a model that can, for example, predict that an algorithm will perform poorly on a new architecture because the memory allocations are to small or because the network is to slow. To do this, the project team will generate the linear solver training data set on a range of different architectures. Using that data from the various machines, the user will create a single model whereby the linear solver features as well as the machines internal specifications are used as the features. 

To futher enable efficient reuse of data we will also investigate the development of Baysian machine learning models, specifically Baysian transfer learning methods. A major assumption in the machine learning models used in previous work is that the training data must be in the identical feature space and distribution. That is to say, the training data must come from the same domain. This is less than ideal in situations where limited training data is available because it prohibits the reuse of existing data in the new domain. In contrast, transfer learning leads to models capable of simultaneously learning form different source domains, making it possible to transfer relevant knowledge from domains with plenty of labeled data to 
new domains where training data is limited. A good analogy for transfer based learning is that of learning a new programming language, whereby the transfer of core skills from previous languages acts to reduce the learning curve associated with the new language.

In the context of SASI, the means using data from a class of algorithms (such as linear solvers) and architecture,  to improve prediction in a similar algorithm class or on another architecture. The Phase I use pre-existing training data sets for linear solvers and eigensolvers to test these new algorithms
   
\subsection{The SASI API} 
Gluing all these tools as algorithms together will be the primary SASI API. The SASI interface will be written in C++ using template parameters and virtual functions to create an interface that is widely applicable across a broad range of numerical applications and simulations. The implementation of the interface will itself form the guiding star for users looking to build SASI models for new algorithms and simulations. 

Several components from the core SolverSelector API will be 

Implementation of the interface will require the user to specify a list of functions for tasks that must be completed to utilize the various tools given inside SASI. All interaction between the domain scientist and the internal SASI tools will go through this interface, with every effort being made to minimize the required number of function definitions. If a function definition is not provided, any tools using that method will be unavailable, but other methods will still be accessible. All in, the SASI API will provide the user with a single place for interacting with and user the numerous SASI based tools. 

 
 
\section{Performance Schedule and Task Plan}
\label{sec:taskplan}

The goal of the Phase I effort will be to provide the reviewers with a clear idea 
of the scientific and commercial potential of SASI. The research and development topics 
described in Section \ref{sec:approach} will be addressed by the tasks described in the remainder of this section. 
Figure~\ref{fig:tasks} summarizes, at a high level, the dependencies among tasks 
and approximate anticipated task durations. The project duration is roughly 
divided into 1 month blocks. Specific details are included in the description 
of each task. 

RNET would like to present the project ideas and research plan to the DOE 
Program Manager and other interested scientists interested in performance tuning
through smart algorithm selection. This meeting will 
be scheduled soon after the Phase I contract is awarded. The Kickoff meeting 
will coincide with the Phase I SBIR PI meeting being hosted by the DOE. The 
meeting can be hosted at RNET, a DOE site suggested by the Program Manager or 
via a teleconference. RNET will submit a final report and present the report 
details along with a Phase II work plan to the DOE program manager and other 
interested scientists.

%\begin{figure}
%\centering
%\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
%   \hline 
%   \multirow{2}{*}{ Time (Months): } & \multicolumn{9}{|c|}{ Phase I } \\
%   \cline{2-10} 
%   & 1 & 2 & 3 & 4 & 5 & 6 & 7 &8 & 9 \\
%   \hline 
%   \bcolumn{Task 1} & X & X & X & X & X  &   &   &   & \\
%   \hline
%   \bcolumn{Task 2} &   &  &  & X & X & X & X  & X  &  \\
%  \hline
%   \bcolumn{Task 3} &   &   &   &  &  & X & X &  X & X \\
%   \hline
%
%\end{tabular}
%\caption{Overview of task dependencies and time-line.}
%\label{fig:tasks}
%\end{figure}

\newcounter{taskCount}
\setcounter{taskCount}{0}

\refstepcounter{taskCount}\label{task:1}
\subsection{Task \ref{task:1}: Generalization and extension of the SolverSelector Framework}
In this task, the project team will begin the process of generalizing the SolverSelector framework for 
use with generic algorithms and simulations. This will include a large litriture review focusing on the methods 
and interfaces used to solve a variety of subproblems in many of the more prominent solver packages (i.e., TODO for 
graph algorithms, TODO for optimization problems, TODO for ODE solvers, etc). At the end of this task, the software interfaces
and indirections for building and integration of SASI models will be completed. All remaining work will look to add functionality, verification, analysis and usability features to the overall toolkit


\refstepcounter{taskCount}\label{task:2}
\subsection{Task \ref{task:2}: Model Verification Toolkit }
In this task, the project team will develop the tools required for model verification, including the feature and data set analysis tools outline in Section~\ref{TODO}. Initial testing of these tools will be completed using the datasets obtained for linear solvers as part of the SolverSelector 
project. This data provides a good baseline for testing as it has been shown to be effective in informing an accurate SASI based model for linear solvers. The tools developed in this task will also be used in task~\ref{task:4} to verify the suitability of the graph algorithm data. 


\refstepcounter{taskCount}\label{task:3}
\subsection{Task \ref{task:3}: }
In this task, the project team will investigate approaches for the re-purposing training data obtained on other architectures and/or from alternative algorithms or application areas in new models and for new architectures. As outlined in Section~\ref{TODO}, this will include an investigation into using data obtained from similar algorithm classes and Baysian methods to kick-start models for new algorithms. To test these methods, RNET and UO will attempt to kick-start a graph algorithm model utilizing training data obtained from linear solvers. 

In addition, this task will also include an investigation into architecture and implementation features such as memory access patterns and/or the structure of the memory on the given architecture. RNET and UO will use the data sets previously obtained for linear solvers to assess the applicability of the developed methods. In those tests, training data will be obtained on the variety of small scale compute facilities available to the project team as outlined in Section~\ref{TODO}. The investigation will be considered a success when RNET can predict the performance of a algorithm on a different computer without requiring specific training data obtained on that machine and when existing training data from linear solvers can be used to inform an accurate machine model for graph algorithms. As above, testing will be done existing data obtained for linear solvers and graph algorithms, as well as will new data generated on various complete architecture. An accuracy of $90\%$ for the prediction of solver performance across architectures will be considered a success. An accuracy of $90\%$ was chosen because that is sufficient to kickstart a model on a new architecture while dramatically reducing the costs associated with data collection. 

A version of this task will likely extend into any phase II project, with the ultimate goal being to create models that are capable of predicting performance on exascale machines utilizing training data obtained on more moderately sized resources using modest core-counts. As such, the phase II effort will require larger scale testing on terra and peta scale compute resources. Previous research suggests achieving this goal will require a combination of the architecture and implementation backed features developed here with either analytical or heuristic based performance models (also to be investigated in Phase II).  

\refstepcounter{taskCount}\label{task:4}
\subsection{Task \ref{task:4}: Demonstration of SASI using Graph Algorithms }

In this task, RNET and UO will demonstrate the capabilities of the SASI framework by applying 
it to graph algorithms. The solver team has tested smart algorithm selection for graph algorithms 
with good results; however, this was a completely manual process and no SolverSelector like framework was ever built. Using 
the SASI framework, the project team will recreate the graph-algorithm models, leading to the development of a SolverSelector
like API. This will allow the project team to create an informative and instructive proof of concept as to the capabilities 
of the SASI framework. 
