\section{Work Plan} 
\label{sec:approach}
%% {\em Provide an explicit, detailed description of the Phase I research
%%   approach and work to be performed.  Indicate what will be done, by
%%   whom (small business, subcontractors, research institution, or
%%   consultants), where it will be done, and how the work will be
%%   carried out.  If applicant is making a commercial or in-kind
%%   contribution to the project, please describe in detail here.  The
%%   Phase I effort should attempt to determine the technical feasibility
%%   of the proposed concept which, if successful, would provide a firm
%%   basis for the Phase II grant application.

%%   Relate the work plan to the objectives of the proposed project.
%%   Discuss the methods planned to achieve each objective or task
%%   explicitly and in detail.  This section should be a substantial
%%   portion of the total grant application.} 

Development of the SASI framework will be composed of two connected but distinct components; namely, the development of numerous tools, analysis routines and machine learning algorithms required to build, test and validate a SASI model, and the development of the interfaces and software indirections required to link together each of the tools while also guiding the user through the process in a useful, informative and intuitive way that is generic enough to apply across the broad spectrum of numerical algorithms and simulations. In what follows, we outline the specific tools and interfaces that will be developed to reach that objective. 

\subsection{ Model, Data and Feature Set Verification and Analysis }

The number one requirement for the development of an accurate machine learning model is a robust, discriminating and informative set of training data, features and classification metrics. As such, a key component of the SASI framework will be a sophiticated suite of algorithms and tests focused on detecting issue related to a poor set of training data. A decription and motivation for the tools and interfaces that will be included in the first iterations of SASI is given below: 

\subsubsection{Feature Sampling} While we cannot automate the process of feature discovery, we can provide a number of automated methods for feature smaples Feature sampling is the process of removing features without any predictive contribution. In our previous work, feature sampling was completed manually using the many feature evaluation tools available in the Weka toolkit, including the CfsSubsetEval with BestFirst search method, Gain Ratio, Info Gain, Principle Components evaluators with Ranker search method. To that end, SASI will include a tool that, through an interface with Weka, automatically performs feature sampling using a combination of these attribute evaluators. 
An additional requirement of SASI based models is that the features must be computational feasible to calculate at runtime; every second spent extracting features represents a second taken away from the potential performance gains afforded by using SASI. To ensure optimal feature set extraction, SASI will include a optimization routine for determining the cheapest feature set from which we can build a machine learning model with a prescribed accuracy. Initially this completed using a recursive parameter search, with more complex optimization methods being investigated in Phase II. 
  
\subsubsection{Database Management} Management of the large datasets required in machine learning models can be a complex and time consuming task. In addition to simply parsing the data into the machine learning model, one must also be concerned with cleaning, sanitizing and validating the datasets. To ensure this is an easy task, SASI will include a database management component built using Sqlite3. Sqlite3, a robust and well documented implementation of SQL is used in SolverSelector to manage the database and has proven to be very effective. 
 
In addition to this, SASI will include a set of data clean up and validation tools, including mechanisms for detecting classification conflicts, replacing or removing missing data, rescaling data to fix prescribed ranges and automatic discretization of data into classifications. These are all standard, albeit time consuming tasks, machine learning tasks that might otherwise be overlooked by a novice user.
 
Finally, the SASI database management tool will include methods for determining data set coverage. Similar to code coverage metrics, this tool will create a visual set of plots highlighting areas of the feature set parameter space where there is a significant lack of data.  Similar to the determination of domain specific features, the process of determining the problems and applications to include in the initial training data set is a process that cannot be automated. However, using this tool, users will be able to direct there data collection efforts toward problems for which data is sparse, rather than repeatedly collecting data that will have little statistical impact on overall performance. 
  
\subsubsection{The SASI API} 
Gluing all these tools as algorithms together will be the primary SASI API. The SASI interface will be written in C++ using template parameters and virtual functions to create an interface that is widely applicable across a broad range of numerical applications and simulations. The implementation of the interface will itself form the guiding star for users looking to build SASI models for new algorithms and simulations. 

Implementation of the interface will require the user to specify a list of functions for tasks that must be completed to utilize the various tools given inside SASI. All interaction between the domain scientist and the internal SASI tools will go through this interface, with every effort being made to minimize the required number of function definitions. If a function definition is not provided, any tools using that method will be unavailable, but other methods will still be accessible. All in, the SASI API will provide the user with a single place for interacting with and user the numerous SASI based tools. 

\subsection{ 

\subsection{Feature Set Detection}

\subsection{ Phase II Tasks: Efficient Data Extraction and Management of HPC resources } 


\subsection{ Model, data and feature set verification and analysis } 


SASI will also attempt to minimize instances of conflicting classifications. Conflicting classification occurs when two separate matrices have the same feature set (and are thereby identical as far as the model is concerned), but are classified differently for a given solver. Conflicts such as these must be minimized, either by adding new or changing existing features,  to ensure the feature set exhibits the discriminating qualities required for use in high accuracy machine learning models. 


\subsection{ Re-purposing training data for use on other applications and architectures }



\section{Performance Schedule and Task Plan}
\label{sec:taskplan}

The goal of the Phase I effort will be to provide the reviewers with a clear idea 
of the scientific and commercial potential of SASI. The research and development topics 
described in Section \ref{sec:approach} will be addressed by the tasks described in the remainder of this section. 
Figure~\ref{fig:tasks} summarizes, at a high level, the dependencies among tasks 
and approximate anticipated task durations. The project duration is roughly 
divided into 1 month blocks. Specific details are included in the description 
of each task. 

RNET would like to present the project ideas and research plan to the DOE 
Program Manager and other interested scientists interested in performance tuning
through smart algorithm selection. This meeting will 
be scheduled soon after the Phase I contract is awarded. The Kickoff meeting 
will coincide with the Phase I SBIR PI meeting being hosted by the DOE. The 
meeting can be hosted at RNET, a DOE site suggested by the Program Manager or 
via a teleconference. RNET will submit a final report and present the report 
details along with a Phase II work plan to the DOE program manager and other 
interested scientists.

%\begin{figure}
%\centering
%\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
%   \hline 
%   \multirow{2}{*}{ Time (Months): } & \multicolumn{9}{|c|}{ Phase I } \\
%   \cline{2-10} 
%   & 1 & 2 & 3 & 4 & 5 & 6 & 7 &8 & 9 \\
%   \hline 
%   \bcolumn{Task 1} & X & X & X & X & X  &   &   &   & \\
%   \hline
%   \bcolumn{Task 2} &   &  &  & X & X & X & X  & X  &  \\
%  \hline
%   \bcolumn{Task 3} &   &   &   &  &  & X & X &  X & X \\
%   \hline
%
%\end{tabular}
%\caption{Overview of task dependencies and time-line.}
%\label{fig:tasks}
%\end{figure}

\newcounter{taskCount}
\setcounter{taskCount}{0}

\refstepcounter{taskCount}\label{task:1}
\subsection{Task \ref{task:1}: Generalization and extension of the SolverSelector Framework}
In this task, the project team will begin the process of generalizing the SolverSelector framework for 
use with generic algorithms and simulations. This will include a large litriture review focusing on the methods 
and interfaces used to solve a variety of subproblems in many of the more prominent solver packages (i.e., TODO for 
graph algorithms, TODO for optimization problems, TODO for ODE solvers, etc). At the end of this task, the software interfaces
and indirections for building and integration of SASI models will be completed. All remaining work will look to add functionality, verification, analysis and usability features to the overall toolkit


\refstepcounter{taskCount}\label{task:2}
\subsection{Task \ref{task:2}: Model Verification Toolkit }
In this task, the project team will develop the tools required for model verification, including the feature and data set analysis tools outline in Section~\ref{TODO}. Initial testing of these tools will be completed using the datasets obtained for linear solvers as part of the SolverSelector 
project. This data provides a good baseline for testing as it has been shown to be effective in informing an accurate SASI based model for linear solvers. The tools developed in this task will also be used in task~\ref{task:4} to verify the suitability of the graph algorithm data. 


\refstepcounter{taskCount}\label{task:3}
\subsection{Task \ref{task:3}: }
In this task, the project team will investigate approaches for the re-purposing training data obtained on other architectures and/or from alternative algorithms or application areas in new models and for new architectures. As outlined in Section~\ref{TODO}, this will include an investigation into using data obtained from similar algorithm classes and Baysian methods to kick-start models for new algorithms. To test these methods, RNET and UO will attempt to kick-start a graph algorithm model utilizing training data obtained from linear solvers. 

In addition, this task will also include an investigation into architecture and implementation features such as memory access patterns and/or the structure of the memory on the given architecture. RNET and UO will use the data sets previously obtained for linear solvers to assess the applicability of the developed methods. In those tests, training data will be obtained on the variety of small scale compute facilities available to the project team as outlined in Section~\ref{TODO}. The investigation will be considered a success when RNET can predict the performance of a algorithm on a different computer without requiring specific training data obtained on that machine and when existing training data from linear solvers can be used to inform an accurate machine model for graph algorithms. As above, testing will be done existing data obtained for linear solvers and graph algorithms, as well as will new data generated on various complete architecture. An accuracy of $90\%$ for the prediction of solver performance across architectures will be considered a success. An accuracy of $90\%$ was chosen because that is sufficient to kickstart a model on a new architecture while dramatically reducing the costs associated with data collection. 

A version of this task will likely extend into any phase II project, with the ultimate goal being to create models that are capable of predicting performance on exascale machines utilizing training data obtained on more moderately sized resources using modest core-counts. As such, the phase II effort will require larger scale testing on terra and peta scale compute resources. Previous research suggests achieving this goal will require a combination of the archtecture and implementation backed features developed here with either analytical or heuristic based performance models (also to be investigated in Phase II).  

\refstepcounter{taskCount}\label{task:4}
\subsection{Task \ref{task:4}: Development of SASI model for Graph Algorithms }

In this task, RNET and UO will demonstrate the capabilities of the SASI framework by applying 
it to graph algorithms. The solver team has tested smart algorithm selection for graph algorithms 
with good results; however, this was a completely manual process and no SolverSelector like framework was ever built. Using 
the SASI framework, the project team will recreate the graph-algorithm models, leading to the development of a SolverSelector
like API. This will allow the project team to create an informative and instructive proof of concept as to the capabilities 
of the SASI framework. 
